% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.1 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\datalist[entry]{nyt/global//global/global}
  \entry{axioms-2}{misc}{}
    \name{author}{3}{}{%
      {{hash=BS}{%
         family={Buchholz},
         familyi={B\bibinitperiod},
         given={Simon},
         giveni={S\bibinitperiod},
      }}%
      {{hash=PJ}{%
         family={Park},
         familyi={P\bibinitperiod},
         given={Junhyung},
         giveni={J\bibinitperiod},
      }}%
      {{hash=SB}{%
         family={Sch√∂lkopf},
         familyi={S\bibinitperiod},
         given={Bernhard},
         giveni={B\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {arXiv}%
    }
    \keyw{FOS: Mathematics, Statistics Theory (math.ST)}
    \strng{namehash}{BSPJSB1}
    \strng{fullhash}{BSPJSB1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{BPS24}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{abstract}{%
    Causal spaces have recently been introduced as a measure-theoretic
  framework to encode the notion of causality. While it has some advantages
  over established frameworks, such as structural causal models, the theory is
  so far only developed for single causal spaces. In many mathematical
  theories, not least the theory of probability spaces of which causal spaces
  are a direct extension, combinations of objects and maps between objects form
  a central part. In this paper, taking inspiration from such objects in
  probability theory, we propose the definitions of products of causal spaces,
  as well as (stochastic) transformations between causal spaces. In the context
  of causality, these quantities can be given direct semantic interpretations
  as causally independent components, abstractions and extensions.%
    }
    \verb{doi}
    \verb 10.48550/ARXIV.2406.00388
    \endverb
    \field{title}{Products, {Abstractions} and {Inclusions} of {Causal}
  {Spaces}}
    \verb{url}
    \verb https://arxiv.org/abs/2406.00388
    \endverb
    \field{year}{2024}
    \field{urlday}{10}
    \field{urlmonth}{05}
    \field{urlyear}{2025}
  \endentry

  \entry{fritz_d-separation_2023}{article}{}
    \name{author}{2}{}{%
      {{hash=FT}{%
         family={Fritz},
         familyi={F\bibinitperiod},
         given={Tobias},
         giveni={T\bibinitperiod},
      }}%
      {{hash=KA}{%
         family={Klingler},
         familyi={K\bibinitperiod},
         given={Andreas},
         giveni={A\bibinitperiod},
      }}%
    }
    \strng{namehash}{FTKA1}
    \strng{fullhash}{FTKA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{FK23}
    \field{sortinit}{F}
    \field{sortinithash}{F}
    \field{abstract}{%
    The d-separation criterion detects the compatibility of a joint probability
  distribution with a directed acyclic graph through certain conditional
  independences. In this work, we study this problem in the context of
  categorical probability theory by introducing a categorical definition of
  causal models, a categorical notion of d-separation, and proving an abstract
  version of the d-separation criterion. This approach has two main benefits.
  First, categorical d-separation is a very intuitive criterion based on
  topological connectedness. Second, our results apply both to
  measure-theoretic probability (with standard Borel spaces) and beyond
  probability theory, including to deterministic and possibilistic networks. It
  therefore provides a clean proof of the equivalence of local and global
  Markov properties with causal compatibility for continuous and mixed random
  variables as well as deterministic and possibilistic variables.%
    }
    \field{issn}{1533-7928}
    \field{number}{46}
    \field{pages}{1\bibrangedash 49}
    \field{title}{The d-Separation Criterion in Categorical Probability}
    \verb{url}
    \verb http://jmlr.org/papers/v24/22-0916.html
    \endverb
    \field{volume}{24}
    \field{journaltitle}{Journal of Machine Learning Research}
    \field{year}{2023}
    \field{urlday}{01}
    \field{urlmonth}{08}
    \field{urlyear}{2025}
  \endentry

  \entry{geiger_causal_2021}{misc}{}
    \name{author}{4}{}{%
      {{hash=GA}{%
         family={Geiger},
         familyi={G\bibinitperiod},
         given={Atticus},
         giveni={A\bibinitperiod},
      }}%
      {{hash=LH}{%
         family={Lu},
         familyi={L\bibinitperiod},
         given={Hanson},
         giveni={H\bibinitperiod},
      }}%
      {{hash=IT}{%
         family={Icard},
         familyi={I\bibinitperiod},
         given={Thomas},
         giveni={T\bibinitperiod},
      }}%
      {{hash=PC}{%
         family={Potts},
         familyi={P\bibinitperiod},
         given={Christopher},
         giveni={C\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {arXiv}%
    }
    \keyw{Artificial Intelligence (cs.AI), FOS: Computer and information
  sciences, Machine Learning (cs.LG)}
    \strng{namehash}{GA+1}
    \strng{fullhash}{GALHITPC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Gei+21}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \field{abstract}{%
    Structural analysis methods (e.g., probing and feature attribution) are
  increasingly important tools for neural network analysis. We propose a new
  structural analysis method grounded in a formal theory of causal abstraction
  that provides rich characterizations of model-internal representations and
  their roles in input/output behavior. In this method, neural representations
  are aligned with variables in interpretable causal models, and then
  interchange interventions are used to experimentally verify that the neural
  representations have the causal properties of their aligned variables. We
  apply this method in a case study to analyze neural models trained on
  Multiply Quantified Natural Language Inference (MQNLI) corpus, a highly
  complex NLI dataset that was constructed with a tree-structured natural logic
  causal model. We discover that a BERT-based model with state-of-the-art
  performance successfully realizes parts of the natural logic model's causal
  structure, whereas a simpler baseline model fails to show any such structure,
  demonstrating that BERT representations encode the compositional structure of
  MQNLI.%
    }
    \verb{doi}
    \verb 10.48550/ARXIV.2106.02997
    \endverb
    \field{title}{Causal {Abstractions} of {Neural} {Networks}}
    \verb{url}
    \verb https://arxiv.org/abs/2106.02997
    \endverb
    \field{year}{2021}
  \endentry

  \entry{geiger_causal_2025}{article}{}
    \name{author}{11}{}{%
      {{hash=GA}{%
         family={Geiger},
         familyi={G\bibinitperiod},
         given={Atticus},
         giveni={A\bibinitperiod},
      }}%
      {{hash=ID}{%
         family={Ibeling},
         familyi={I\bibinitperiod},
         given={Duligur},
         giveni={D\bibinitperiod},
      }}%
      {{hash=ZA}{%
         family={Zur},
         familyi={Z\bibinitperiod},
         given={Amir},
         giveni={A\bibinitperiod},
      }}%
      {{hash=CM}{%
         family={Chaudhary},
         familyi={C\bibinitperiod},
         given={Maheep},
         giveni={M\bibinitperiod},
      }}%
      {{hash=CS}{%
         family={Chauhan},
         familyi={C\bibinitperiod},
         given={Sonakshi},
         giveni={S\bibinitperiod},
      }}%
      {{hash=HJ}{%
         family={Huang},
         familyi={H\bibinitperiod},
         given={Jing},
         giveni={J\bibinitperiod},
      }}%
      {{hash=AA}{%
         family={Arora},
         familyi={A\bibinitperiod},
         given={Aryaman},
         giveni={A\bibinitperiod},
      }}%
      {{hash=WZ}{%
         family={Wu},
         familyi={W\bibinitperiod},
         given={Zhengxuan},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=GN}{%
         family={Goodman},
         familyi={G\bibinitperiod},
         given={Noah},
         giveni={N\bibinitperiod},
      }}%
      {{hash=PC}{%
         family={Potts},
         familyi={P\bibinitperiod},
         given={Christopher},
         giveni={C\bibinitperiod},
      }}%
      {{hash=IT}{%
         family={Icard},
         familyi={I\bibinitperiod},
         given={Thomas},
         giveni={T\bibinitperiod},
      }}%
    }
    \strng{namehash}{GA+1}
    \strng{fullhash}{GAIDZACMCSHJAAWZGNPCIT1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{Gei+25}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \field{abstract}{%
    Causal abstraction provides a theoretical foundation for mechanistic
  interpretability, the field concerned with providing intelligible algorithms
  that are faithful simplifications of the known, but opaque low-level details
  of black box AI models. Our contributions are (1) generalizing the theory of
  causal abstraction from mechanism replacement (i.e., hard and soft
  interventions) to arbitrary mechanism transformation (i.e., functionals from
  old mechanisms to new mechanisms), (2) providing a flexible, yet precise
  formalization for the core concepts of polysemantic neurons, the linear
  representation hypothesis, modular features, and graded faithfulness, and (3)
  unifying a variety of mechanistic interpretability methods in the common
  language of causal abstraction, namely, activation and path patching, causal
  mediation analysis, causal scrubbing, causal tracing, circuit analysis,
  concept erasure, sparse autoencoders, differential binary masking,
  distributed alignment search, and steering.%
    }
    \field{issn}{1533-7928}
    \field{number}{83}
    \field{pages}{1\bibrangedash 64}
    \field{shorttitle}{Causal {Abstraction}}
    \field{title}{Causal {Abstraction}: {A} {Theoretical} {Foundation} for
  {Mechanistic} {Interpretability}}
    \verb{url}
    \verb http://jmlr.org/papers/v26/23-0058.html
    \endverb
    \field{volume}{26}
    \field{journaltitle}{Journal of Machine Learning Research}
    \field{year}{2025}
  \endentry

  \entry{huber_introduction_2024}{article}{}
    \name{author}{1}{}{%
      {{hash=HM}{%
         family={Huber},
         familyi={H\bibinitperiod},
         given={Martin},
         giveni={M\bibinitperiod},
      }}%
    }
    \strng{namehash}{HM1}
    \strng{fullhash}{HM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Hub24}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{abstract}{%
    Abstract In social sciences and economics, causal inference traditionally
  focuses on assessing the impact of predefined treatments (or interventions)
  on predefined outcomes, such as the effect of education programs on earnings.
  Causal discovery, in contrast, aims to uncover causal relationships among
  multiple variables in a data-driven manner, by investigating statistical
  associations rather than relying on predefined causal structures. This
  approach, more common in computer science, seeks to understand causality in
  an entire system of variables, which can be visualized by causal graphs. This
  survey provides an introduction to key concepts, algorithms, and applications
  of causal discovery from the perspectives of economics and social sciences.
  It covers fundamental concepts like d-separation, causal faithfulness, and
  Markov equivalence, sketches various algorithms for causal discovery and
  discusses the back-door and front-door criteria for identifying causal
  effects. The survey concludes with more specific examples of causal
  discovery, e.g., for learning all variables that directly affect an outcome
  of interest and/or testing identification of causal effects in observational
  data.%
    }
    \verb{doi}
    \verb 10.1186/s41937-024-00131-4
    \endverb
    \field{issn}{2235-6282}
    \field{number}{1}
    \field{pages}{14}
    \field{shortjournal}{Swiss J Economics Statistics}
    \field{title}{An introduction to causal discovery}
    \verb{url}
    \verb https://sjes.springeropen.com/articles/10.1186/s41937-024-00131-4
    \endverb
    \field{volume}{160}
    \field{langid}{english}
    \field{journaltitle}{Swiss Journal of Economics and Statistics}
    \field{day}{29}
    \field{month}{10}
    \field{year}{2024}
  \endentry

  \entry{bojanczyk_causal_2019}{incollection}{}
    \name{author}{3}{}{%
      {{hash=JB}{%
         family={Jacobs},
         familyi={J\bibinitperiod},
         given={Bart},
         giveni={B\bibinitperiod},
      }}%
      {{hash=KA}{%
         family={Kissinger},
         familyi={K\bibinitperiod},
         given={Aleks},
         giveni={A\bibinitperiod},
      }}%
      {{hash=ZF}{%
         family={Zanasi},
         familyi={Z\bibinitperiod},
         given={Fabio},
         giveni={F\bibinitperiod},
      }}%
    }
    \name{editor}{2}{}{%
      {{hash=BM}{%
         family={Boja≈Ñczyk},
         familyi={B\bibinitperiod},
         given={Miko≈Çaj},
         giveni={M\bibinitperiod},
      }}%
      {{hash=SA}{%
         family={Simpson},
         familyi={S\bibinitperiod},
         given={Alex},
         giveni={A\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer International Publishing}%
    }
    \strng{namehash}{JBKAZF1}
    \strng{fullhash}{JBKAZF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{JKZ19}
    \field{sortinit}{J}
    \field{sortinithash}{J}
    \field{abstract}{%
    Abstract Extracting causal relationships from observed correlations is a
  growing area in probabilistic reasoning, originating with the seminal work of
  Pearl and others from the early 1990s. This paper develops a new,
  categorically oriented view based on a clear distinction between syntax
  (string diagrams) and semantics (stochastic matrices), connected via
  interpretations as structure-preserving functors. A key notion in the
  identification of causal effects is that of an intervention, whereby a
  variable is forcefully set to a particular value independent of any prior
  dependencies. We represent the effect of such an intervention as an
  endofunctor which performs ‚Äòstring diagram surgery‚Äô within the syntactic
  category of string diagrams. This diagram surgery in turn yields a new,
  interventional distribution via the interpretation functor. While in general
  there is no way to compute interventional distributions purely from observed
  data, we show that this is possible in certain special cases using a
  calculational tool called comb disintegration. We showcase this technique on
  a well-known example, predicting the causal effect of smoking on cancer in
  the presence of a confounding common cause. We then conclude by showing that
  this technique provides simple sufficient conditions for computing
  interventions which apply to a wide variety of situations considered in the
  causal inference literature.%
    }
    \field{booktitle}{Foundations of Software Science and Computation
  Structures}
    \verb{doi}
    \verb 10.1007/978-3-030-17127-8_18
    \endverb
    \field{isbn}{9783030171261 9783030171278}
    \field{pages}{313\bibrangedash 329}
    \field{title}{Causal Inference by String Diagram Surgery}
    \verb{url}
    \verb https://link.springer.com/10.1007/978-3-030-17127-8_18
    \endverb
    \field{volume}{11425}
    \field{langid}{english}
    \list{location}{1}{%
      {Cham}%
    }
    \field{year}{2019}
    \field{urlday}{01}
    \field{urlmonth}{08}
    \field{urlyear}{2025}
  \endentry

  \entry{lawvereprobability1962}{article}{}
    \name{author}{1}{}{%
      {{hash=LW}{%
         family={Lawvere},
         familyi={L\bibinitperiod},
         given={William},
         giveni={W\bibinitperiod},
      }}%
    }
    \strng{namehash}{LW1}
    \strng{fullhash}{LW1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Law62}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{title}{The Category of Probabilistic Maps}
    \verb{url}
    \verb https://ncatlab.org/nlab/files/lawvereprobability1962.pdf
    \endverb
    \field{year}{1962}
  \endentry

  \entry{lorenz_causal_2023}{misc}{}
    \name{author}{2}{}{%
      {{hash=LR}{%
         family={Lorenz},
         familyi={L\bibinitperiod},
         given={Robin},
         giveni={R\bibinitperiod},
      }}%
      {{hash=TS}{%
         family={Tull},
         familyi={T\bibinitperiod},
         given={Sean},
         giveni={S\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {arXiv}%
    }
    \keyw{Computer Science - Logic in Computer Science, Computer Science -
  Machine Learning, Mathematics - Category Theory}
    \strng{namehash}{LRTS1}
    \strng{fullhash}{LRTS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{LT23}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{abstract}{%
    The framework of causal models provides a principled approach to causal
  reasoning, applied today across many scientific domains. Here we present this
  framework in the language of string diagrams, interpreted formally using
  category theory. A class of string diagrams, called network diagrams, are in
  1-to-1 correspondence with directed acyclic graphs. A causal model is given
  by such a diagram with its components interpreted as stochastic maps,
  functions, or general channels in a symmetric monoidal category with a
  'copy-discard' structure (cd-category), turning a model into a single
  mathematical object that can be reasoned with intuitively and yet rigorously.
  Building on prior works by Fong and Jacobs, Kissinger and Zanasi, as well as
  Fritz and Klingler, we present diagrammatic definitions of causal models and
  functional causal models in a cd-category, generalising causal Bayesian
  networks and structural causal models, respectively. We formalise general
  interventions on a model, including but beyond do-interventions, and present
  the natural notion of an open causal model with inputs. We also give an
  approach to conditioning based on a normalisation box, allowing for causal
  inference calculations to be done fully diagrammatically. We define
  counterfactuals in this setup, and treat the problems of the identifiability
  of causal effects and counterfactuals fully diagrammatically. The benefits of
  such a presentation of causal models lie in foundational questions in causal
  reasoning and in their clarificatory role and pedagogical value. This work
  aims to be accessible to different communities, from causal model
  practitioners to researchers in applied category theory, and discusses many
  examples from the literature for illustration. Overall, we argue and
  demonstrate that causal reasoning according to the causal model framework is
  most naturally and intuitively done as diagrammatic reasoning.%
    }
    \verb{doi}
    \verb 10.48550/arXiv.2304.07638
    \endverb
    \field{note}{arXiv:2304.07638}
    \field{title}{Causal models in string diagrams}
    \verb{url}
    \verb http://arxiv.org/abs/2304.07638
    \endverb
    \field{month}{04}
    \field{year}{2023}
  \endentry

  \entry{mac_lane_categories_1978}{book}{}
    \name{author}{1}{}{%
      {{hash=MLS}{%
         family={Mac\bibnamedelima Lane},
         familyi={M\bibinitperiod\bibinitdelim L\bibinitperiod},
         given={Saunders},
         giveni={S\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer New York}%
    }
    \strng{namehash}{MLS1}
    \strng{fullhash}{MLS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{ML78}
    \field{sortinit}{M}
    \field{sortinithash}{M}
    \verb{doi}
    \verb 10.1007/978-1-4757-4721-8
    \endverb
    \field{isbn}{9781441931238 9781475747218}
    \field{series}{Graduate Texts in Mathematics}
    \field{title}{Categories for the Working Mathematician}
    \verb{url}
    \verb http://link.springer.com/10.1007/978-1-4757-4721-8
    \endverb
    \field{volume}{5}
    \list{location}{1}{%
      {New York, {NY}}%
    }
    \field{year}{1978}
  \endentry

  \entry{axioms-1}{misc}{}
    \name{author}{4}{}{%
      {{hash=PJ}{%
         family={Park},
         familyi={P\bibinitperiod},
         given={Junhyung},
         giveni={J\bibinitperiod},
      }}%
      {{hash=BS}{%
         family={Buchholz},
         familyi={B\bibinitperiod},
         given={Simon},
         giveni={S\bibinitperiod},
      }}%
      {{hash=SB}{%
         family={Sch√∂lkopf},
         familyi={S\bibinitperiod},
         given={Bernhard},
         giveni={B\bibinitperiod},
      }}%
      {{hash=MK}{%
         family={Muandet},
         familyi={M\bibinitperiod},
         given={Krikamol},
         giveni={K\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {arXiv}%
    }
    \keyw{Computer Science - Artificial Intelligence, Mathematics - Statistics
  Theory, Statistics - Statistics Theory}
    \strng{namehash}{PJ+1}
    \strng{fullhash}{PJBSSBMK1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Par+24}
    \field{sortinit}{P}
    \field{sortinithash}{P}
    \field{abstract}{%
    Causality is a central concept in a wide range of research areas, yet there
  is still no universally agreed axiomatisation of causality. We view causality
  both as an extension of probability theory and as a study of
  {\textbackslash}textit\{what happens when one intervenes on a system\}, and
  argue in favour of taking Kolmogorov's measure-theoretic axiomatisation of
  probability as the starting point towards an axiomatisation of causality. To
  that end, we propose the notion of a {\textbackslash}textit\{causal space\},
  consisting of a probability space along with a collection of transition
  probability kernels, called {\textbackslash}textit\{causal kernels\}, that
  encode the causal information of the space. Our proposed framework is not
  only rigorously grounded in measure theory, but it also sheds light on
  long-standing limitations of existing frameworks including, for example,
  cycles, latent variables and stochastic processes.%
    }
    \verb{doi}
    \verb 10.48550/arXiv.2305.17139
    \endverb
    \field{note}{arXiv:2305.17139}
    \field{title}{A {Measure}-{Theoretic} {Axiomatisation} of {Causality}}
    \verb{url}
    \verb http://arxiv.org/abs/2305.17139
    \endverb
    \field{month}{06}
    \field{year}{2024}
  \endentry

  \entry{Jonas_Peters_mini}{misc}{}
    \name{author}{1}{}{%
      {{hash=PJ}{%
         family={Peters},
         familyi={P\bibinitperiod},
         given={Jonas},
         giveni={J\bibinitperiod},
      }}%
    }
    \strng{namehash}{PJ1}
    \strng{fullhash}{PJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Pet17}
    \field{sortinit}{P}
    \field{sortinithash}{P}
    \field{abstract}{%
    Machine learning expert Jonas Peters of the University of Copenhagen
  presents ‚ÄúFour Lectures on Causality‚Äù. Produced by the Laboratory for
  Information \& Decision Systems (LIDS) of MIT (https://lids.mit.edu/) and
  Models, Inference \& Algorithms of the Broad Institute
  (https://broadinstitute.org/mia). Most of recent machine learning is focused
  on pure predictive performance, which has been a driving force behind its
  practical success. The question of causality (understanding why predictions
  work) has been somewhat left behind. This paradigm is incredibly important,
  because it can help understand things like which genes cause which diseases,
  and which policy affects which economic indicator, for example. In the field
  of causality we want to understand how a system reacts under interventions
  (e.g. in gene knock-out experiments). These questions go beyond statistical
  dependences and can therefore not be answered by standard regression or
  classification techniques. In this tutorial you will learn about the
  interesting problem of causal inference and recent developments in the field.
  No prior knowledge about causality is required. Part 1: We introduce
  structural causal models and formalize interventional distributions. We
  define causal effects and show how to compute them if the causal structure is
  known.%
    }
    \field{title}{Lectures on {Causality}: {Jonas} {Peters}, {Part} 1}
    \verb{url}
    \verb https://www.youtube.com/watch?v=zvrcyqcN9Wo&list=PLW01hpWnEtbTcuY0a0j
    \verb hZyanHX3GPImAy&index=1
    \endverb
    \field{month}{03}
    \field{year}{2017}
  \endentry

  \entry{elements_causal_inf}{book}{}
    \name{author}{3}{}{%
      {{hash=PJ}{%
         family={Peters},
         familyi={P\bibinitperiod},
         given={Jonas},
         giveni={J\bibinitperiod},
      }}%
      {{hash=JD}{%
         family={Janzing},
         familyi={J\bibinitperiod},
         given={Dominik},
         giveni={D\bibinitperiod},
      }}%
      {{hash=SB}{%
         family={Sch√∂lkopf},
         familyi={S\bibinitperiod},
         given={Bernhard},
         giveni={B\bibinitperiod},
      }}%
    }
    \list{language}{1}{%
      {eng}%
    }
    \list{publisher}{1}{%
      {The MIT press}%
    }
    \strng{namehash}{PJJDSB1}
    \strng{fullhash}{PJJDSB1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{PJS17}
    \field{sortinit}{P}
    \field{sortinithash}{P}
    \field{abstract}{%
    "The mathematization of causality is a relatively recent development, and
  has become increasingly important in data science and machine learning. This
  book offers a self-contained and concise introduction to causal models and
  how to learn them from data"--Back of book%
    }
    \field{isbn}{9780262037310}
    \field{series}{Adaptive computation and machine learning}
    \field{shorttitle}{Elements of causal inference}
    \field{title}{Elements of causal inference: foundations and learning
  algorithms}
    \list{location}{1}{%
      {Cambridge, Mass}%
    }
    \field{year}{2017}
  \endentry

  \entry{templeton2024scaling}{article}{}
    \name{author}{22}{}{%
      {{hash=TA}{%
         family={Templeton},
         familyi={T\bibinitperiod},
         given={Adly},
         giveni={A\bibinitperiod},
      }}%
      {{hash=CT}{%
         family={Conerly},
         familyi={C\bibinitperiod},
         given={Tom},
         giveni={T\bibinitperiod},
      }}%
      {{hash=MJ}{%
         family={Marcus},
         familyi={M\bibinitperiod},
         given={Jonathan},
         giveni={J\bibinitperiod},
      }}%
      {{hash=LJ}{%
         family={Lindsey},
         familyi={L\bibinitperiod},
         given={Jack},
         giveni={J\bibinitperiod},
      }}%
      {{hash=BT}{%
         family={Bricken},
         familyi={B\bibinitperiod},
         given={Trenton},
         giveni={T\bibinitperiod},
      }}%
      {{hash=CB}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={Brian},
         giveni={B\bibinitperiod},
      }}%
      {{hash=PA}{%
         family={Pearce},
         familyi={P\bibinitperiod},
         given={Adam},
         giveni={A\bibinitperiod},
      }}%
      {{hash=CC}{%
         family={Citro},
         familyi={C\bibinitperiod},
         given={Craig},
         giveni={C\bibinitperiod},
      }}%
      {{hash=AE}{%
         family={Ameisen},
         familyi={A\bibinitperiod},
         given={Emmanuel},
         giveni={E\bibinitperiod},
      }}%
      {{hash=JA}{%
         family={Jones},
         familyi={J\bibinitperiod},
         given={Andy},
         giveni={A\bibinitperiod},
      }}%
      {{hash=CH}{%
         family={Cunningham},
         familyi={C\bibinitperiod},
         given={Hoagy},
         giveni={H\bibinitperiod},
      }}%
      {{hash=TNL}{%
         family={Turner},
         familyi={T\bibinitperiod},
         given={Nicholas\bibnamedelima L},
         giveni={N\bibinitperiod\bibinitdelim L\bibinitperiod},
      }}%
      {{hash=MC}{%
         family={McDougall},
         familyi={M\bibinitperiod},
         given={Callum},
         giveni={C\bibinitperiod},
      }}%
      {{hash=MM}{%
         family={MacDiarmid},
         familyi={M\bibinitperiod},
         given={Monte},
         giveni={M\bibinitperiod},
      }}%
      {{hash=FCD}{%
         family={Freeman},
         familyi={F\bibinitperiod},
         given={C.\bibnamedelima Daniel},
         giveni={C\bibinitperiod\bibinitdelim D\bibinitperiod},
      }}%
      {{hash=STR}{%
         family={Sumers},
         familyi={S\bibinitperiod},
         given={Theodore\bibnamedelima R.},
         giveni={T\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
      {{hash=RE}{%
         family={Rees},
         familyi={R\bibinitperiod},
         given={Edward},
         giveni={E\bibinitperiod},
      }}%
      {{hash=BJ}{%
         family={Batson},
         familyi={B\bibinitperiod},
         given={Joshua},
         giveni={J\bibinitperiod},
      }}%
      {{hash=JA}{%
         family={Jermyn},
         familyi={J\bibinitperiod},
         given={Adam},
         giveni={A\bibinitperiod},
      }}%
      {{hash=CS}{%
         family={Carter},
         familyi={C\bibinitperiod},
         given={Shan},
         giveni={S\bibinitperiod},
      }}%
      {{hash=OC}{%
         family={Olah},
         familyi={O\bibinitperiod},
         given={Chris},
         giveni={C\bibinitperiod},
      }}%
      {{hash=HT}{%
         family={Henighan},
         familyi={H\bibinitperiod},
         given={Tom},
         giveni={T\bibinitperiod},
      }}%
    }
    \strng{namehash}{TA+1}
    \strng{fullhash}{TACTMJLJBTCBPACCAEJACHTNLMCMMFCDSTRREBJJACSOCHT1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Tem+24}
    \field{sortinit}{T}
    \field{sortinithash}{T}
    \field{title}{Scaling Monosemanticity: Extracting Interpretable Features
  from Claude 3 Sonnet}
    \verb{url}
    \verb https://transformer-circuits.pub/2024/scaling-monosemanticity/index.h
    \verb tml
    \endverb
    \field{journaltitle}{Transformer Circuits Thread}
    \field{year}{2024}
  \endentry

  \entry{triantafillou_predicting_2017}{article}{}
    \name{author}{6}{}{%
      {{hash=TS}{%
         family={Triantafillou},
         familyi={T\bibinitperiod},
         given={Sofia},
         giveni={S\bibinitperiod},
      }}%
      {{hash=LV}{%
         family={Lagani},
         familyi={L\bibinitperiod},
         given={Vincenzo},
         giveni={V\bibinitperiod},
      }}%
      {{hash=HDC}{%
         family={Heinze-Deml},
         familyi={H\bibinithyphendelim D\bibinitperiod},
         given={Christina},
         giveni={C\bibinitperiod},
      }}%
      {{hash=SA}{%
         family={Schmidt},
         familyi={S\bibinitperiod},
         given={Angelika},
         giveni={A\bibinitperiod},
      }}%
      {{hash=TJ}{%
         family={Tegner},
         familyi={T\bibinitperiod},
         given={Jesper},
         giveni={J\bibinitperiod},
      }}%
      {{hash=TI}{%
         family={Tsamardinos},
         familyi={T\bibinitperiod},
         given={Ioannis},
         giveni={I\bibinitperiod},
      }}%
    }
    \strng{namehash}{TS+1}
    \strng{fullhash}{TSLVHDCSATJTI1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{Tri+17}
    \field{sortinit}{T}
    \field{sortinithash}{T}
    \field{abstract}{%
    Abstract Learning the causal relationships that define a molecular system
  allows us to predict how the system will respond to different interventions.
  Distinguishing causality from mere association typically requires randomized
  experiments. Methods for automated¬† causal discovery from limited
  experiments exist, but have so far rarely been tested in systems biology
  applications. In this work, we apply state-of-the art causal discovery
  methods on a large collection of public mass cytometry data sets, measuring
  intra-cellular signaling proteins of the human immune system and their
  response to several perturbations. We show how different experimental
  conditions can be used to facilitate causal discovery, and apply two
  fundamental methods that produce context-specific causal predictions. Causal
  predictions were reproducible across independent data sets from two different
  studies, but often disagree with the {KEGG} pathway databases. Within this
  context, we discuss the caveats we need to overcome for automated causal
  discovery to become a part of the routine data analysis in systems biology.%
    }
    \verb{doi}
    \verb 10.1038/s41598-017-08582-x
    \endverb
    \field{issn}{2045-2322}
    \field{number}{1}
    \field{pages}{12724}
    \field{shortjournal}{Sci Rep}
    \field{shorttitle}{Predicting Causal Relationships from Biological Data}
    \field{title}{Predicting Causal Relationships from Biological Data:
  Applying Automated Causal Discovery on Mass Cytometry Data of Human Immune
  Cells}
    \verb{url}
    \verb https://www.nature.com/articles/s41598-017-08582-x
    \endverb
    \field{volume}{7}
    \field{langid}{english}
    \field{journaltitle}{Scientific Reports}
    \field{day}{05}
    \field{month}{10}
    \field{year}{2017}
    \field{urlday}{01}
    \field{urlmonth}{08}
    \field{urlyear}{2025}
  \endentry

  \entry{tull_towards_2024}{misc}{}
    \name{author}{5}{}{%
      {{hash=TS}{%
         family={Tull},
         familyi={T\bibinitperiod},
         given={Sean},
         giveni={S\bibinitperiod},
      }}%
      {{hash=LR}{%
         family={Lorenz},
         familyi={L\bibinitperiod},
         given={Robin},
         giveni={R\bibinitperiod},
      }}%
      {{hash=CS}{%
         family={Clark},
         familyi={C\bibinitperiod},
         given={Stephen},
         giveni={S\bibinitperiod},
      }}%
      {{hash=KI}{%
         family={Khan},
         familyi={K\bibinitperiod},
         given={Ilyas},
         giveni={I\bibinitperiod},
      }}%
      {{hash=CB}{%
         family={Coecke},
         familyi={C\bibinitperiod},
         given={Bob},
         giveni={B\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {arXiv}%
    }
    \keyw{Computer Science - Artificial Intelligence, Computer Science - Logic
  in Computer Science, Computer Science - Machine Learning, Mathematics -
  Category Theory}
    \strng{namehash}{TS+2}
    \strng{fullhash}{TSLRCSKICB1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Tul+24}
    \field{sortinit}{T}
    \field{sortinithash}{T}
    \field{abstract}{%
    Artificial intelligence (AI) is currently based largely on black-box
  machine learning models which lack interpretability. The field of eXplainable
  AI (XAI) strives to address this major concern, being critical in high-stakes
  areas such as the finance, legal and health sectors. We present an approach
  to defining AI models and their interpretability based on category theory.
  For this we employ the notion of a compositional model, which sees a model in
  terms of formal string diagrams which capture its abstract structure together
  with its concrete implementation. This comprehensive view incorporates
  deterministic, probabilistic and quantum models. We compare a wide range of
  AI models as compositional models, including linear and rule-based models,
  (recurrent) neural networks, transformers, VAEs, and causal and DisCoCirc
  models. Next we give a definition of interpretation of a model in terms of
  its compositional structure, demonstrating how to analyse the
  interpretability of a model, and using this to clarify common themes in XAI.
  We find that what makes the standard 'intrinsically interpretable' models so
  transparent is brought out most clearly diagrammatically. This leads us to
  the more general notion of compositionally-interpretable (CI) models, which
  additionally include, for instance, causal, conceptual space, and DisCoCirc
  models. We next demonstrate the explainability benefits of CI models.
  Firstly, their compositional structure may allow the computation of other
  quantities of interest, and may facilitate inference from the model to the
  modelled phenomenon by matching its structure. Secondly, they allow for
  diagrammatic explanations for their behaviour, based on influence
  constraints, diagram surgery and rewrite explanations. Finally, we discuss
  many future directions for the approach, raising the question of how to learn
  such meaningfully structured models in practice.%
    }
    \verb{doi}
    \verb 10.48550/arXiv.2406.17583
    \endverb
    \field{note}{arXiv:2406.17583}
    \field{title}{Towards {Compositional} {Interpretability} for {XAI}}
    \verb{url}
    \verb http://arxiv.org/abs/2406.17583
    \endverb
    \field{month}{06}
    \field{year}{2024}
  \endentry

  \entry{cinlar}{book}{}
    \name{author}{1}{}{%
      {{hash=√E}{%
         family={√áinlar},
         familyi={√\bibinitperiod},
         given={Erhan},
         giveni={E\bibinitperiod},
      }}%
    }
    \list{language}{1}{%
      {en}%
    }
    \list{publisher}{1}{%
      {Springer New York}%
    }
    \strng{namehash}{√E1}
    \strng{fullhash}{√E1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{√ái11}
    \field{sortinit}{√}
    \field{sortinithash}{√}
    \verb{doi}
    \verb 10.1007/978-0-387-87859-1
    \endverb
    \field{isbn}{9780387878584 9780387878591}
    \field{series}{Graduate {Texts} in {Mathematics}}
    \field{title}{Probability and {Stochastics}}
    \verb{url}
    \verb https://link.springer.com/10.1007/978-0-387-87859-1
    \endverb
    \field{volume}{261}
    \list{location}{1}{%
      {New York, NY}%
    }
    \field{year}{2011}
  \endentry
\enddatalist
\endinput
